<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>new_yomi_approach</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h2 id="approximating-nash-equilibria-through-play">Approximating Nash
equilibria through play</h2>
<p>The trouble with the previous approach was in finding good nash
equilibrium solvers that can solve large state spaces (&gt;10 cards)
quickly.</p>
<p>Instead, we try to approximate something like a Nash equilibrium by
letting pairs of models play against each other and optimising their
strategy mutually and hope that the outcome is close enough to a true
equilibrium state while taking into account the lack of complete
information. This should be possible, see for example <a
href="https://openreview.net/pdf?id=cc8h3I3V4E">this paper</a>. This
means a player does not try to simulate the state of the opponents mind
explicitly, but rather this information is captured by the model.</p>
<p>In this new approach, just like in the old one , we try to
approximate the payout function by training a model to predict given a
game state <span class="math inline">\(s\)</span> what the chance of a
win is <span class="math inline">\(X=1\)</span>,</p>
<p><span class="math display">\[P(X=1|s,\pi_{\mbox{self}}
,\pi_{\mbox{other}}) = f(s) \approx \bar{f}(s)\]</span></p>
<p>where <span class="math inline">\(\pi_{\mbox{self}}
,\pi_{\mbox{other}}\)</span> are the policies of both players, <span
class="math inline">\(\bar{f}(s)\)</span> is the estimator we want to
train and <span class="math inline">\(s\)</span> is the complete state
containing all the knowledge of the player, such as identity of own
cards, number of opponent cards, identity of known opponent cards, hp
and status effects. Such a model is straightforward to train if the
opponent policy stays constant.</p>
<p>We will also need a policy network (though for the full yomi game
more, to model the actions to take for other actions, such as exchanges,
boosting combos and pumping play, and other non–cardplay actions). Such
a model should return a probability distribution over the available
actions (card plays) <span
class="math inline">\(\mathcal{A}\)</span></p>
<p><span class="math display">\[P(a| s, \pi_{\mbox{self}}
,\pi_{\mbox{other}}), a \in \mathcal{A} = g(a,s) =
\bar{\mathbf{g}}(s),\]</span> where <span
class="math inline">\(\bar{\mathbf{g}}\)</span> is a distribution over
the action space. How would such a model be trained?</p>
<p>As we play, we sample from the action distribution and could punish
or reward the model based on the outcome of that particular interaction.
That would however be wasteful, as the model would not have made use of
the actions in would not have chosen.</p>
<p>Instead, we can map for each action taken and the actual action <span
class="math inline">\(z\)</span> by the other player, how the state
would have evolved</p>
<p><span class="math display">\[ \begin{array}{c} s + z + a_1
\rightarrow s_1  \\
s + z + a_2 \rightarrow s_2 \\
\vdots \\
s + z + a_n \rightarrow s_n
\end{array}\]</span></p>
<p>While in principle we don’t know whether which of these states is
best, we can let the previous model decide. We could define a reward
vector in such a way</p>
<p><span class="math display">\[\mathbf{f}: \mathbf{f}_i = \bar{f}(s_i)
- \bar{f}(s),\]</span></p>
<p>or any other monotenous function of the difference, such as</p>
<p><span class="math display">\[\mathbf{f}: \mathbf{f}_i = \log
\bar{f}(s_i) - \log \bar{f}(s),\]</span></p>
<p>One objective could be to maximise the expected reward</p>
<p><span class="math display">\[ \max  \bar{\mathbf{f}}(s)^T
\bar{\mathbf{g}}(s) ,\]</span></p>
<p>or</p>
<p><span class="math display">\[L = -
\bar{\mathbf{f}}(s)^T  \bar{\mathbf{g}}(s) ,\]</span></p>
<p>which can naturally be batched over different iteration situations
<span class="math inline">\(s\)</span>. Having 2 models here is actually
a benefit, since the errors of the models are independent, and so the
gradient should be unbiased.</p>
<p>The research in <a
href="https://openreview.net/pdf?id=cc8h3I3V4E">this paper</a> suggests
that there is benefit, despite there being a different setup due to the
unknown nature of the opponents action choices, to align the gradient
with the tangent space to the simplex <span class="math inline">\(\sum
g_i(s) = 1\)</span>. If we have <span class="math inline">\(m\)</span>
actions available, we can project using a projection operator</p>
<p><span class="math display">\[\Pi = \mathbf{I}_m -
\frac{1}{m}\mathbf{1}_m \mathbf{1}_m^T.\]</span></p>
<p>Since <span class="math inline">\(\Pi = \Pi^T = \Pi^2\)</span> we can
modify the loss to be any of</p>
<p><span class="math display">\[L = -
(\Pi\bar{\mathbf{f}}(s))^T  \bar{\mathbf{g}}(s)  =  -
\bar{\mathbf{f}}(s)^T  (\Pi\bar{\mathbf{g}}(s)) = -
(\Pi\bar{\mathbf{f}}(s))^T (\Pi\bar{\mathbf{g}}(s)), \]</span></p>
<p>which would suggest that the gradient of the loss with respect to the
parametrisation of <span class="math inline">\(\bar{\mathbf{g}}\)</span>
would be a projection as well and leave the simplex intact (though the
model itself would naturally renormalise the outputs to sum to 1).</p>
<h2 id="regret-based-approach">Regret based approach</h2>
<p>While the above reasoning seems in principle sound, in practice the
resulting equilibria are not stable, as small mismatches in the hyper
parameters lead to over controlling and oscillating behaviour in the
space of play.</p>
<p>Instead, we can try for the model to predict the regret per action
taken</p>
<p><span class="math display">\[R(a) =  r(a) - r^*,\]</span> where <span
class="math inline">\(r^*\)</span> is the reward under the best of the
available actions and <span class="math inline">\(r(a)\)</span> is the,
potentially counterfactual, reward we got for that action. Normally
these are added together per action and the strategy of a player updates
as</p>
<p><span class="math display">\[P(a) = \frac{\max(R(a),0)}{\sum_{a&#39;}
\max(a,R(a&#39;))}.\]</span></p>
<h2 id="wolf-learning">WOLF learning</h2>
<p>The problem with the raw regret and performance based update schemes
is that the models tend to overshot in training and the strategies do
not converge to the Nash equilibrium. Instead, we will use the WOLF
strategy, which stands for <em>Win Or Learn Fast</em>. The idea is that
if to keep an average of the historic stategies in memory and compare
them to the current strategy. If the current strategy performs better,
the update step slows down, otherwise it grows. That change in dynamics
is enough to force the Nash equilibrium.</p>
<p>In particular, there are two update speeds, one for winning <span
class="math inline">\(\delta_w\)</span> and one for losing <span
class="math inline">\(\delta_l\)</span>, with <span
class="math inline">\(\delta_l &gt; \delta_w\)</span>. Further there is
an update parameter <span class="math inline">\(\alpha\)</span>. At any
given state <span class="math inline">\(s\)</span>, we try to measure
the value of each available action <span class="math inline">\(a\in
\mathcal{A}.\)</span></p>
<p>If we know the transition function <span class="math inline">\(T(s,a)
\rightarrow s&#39;\)</span>, we can update for each action (even the
unobserved ones, since we know the counterfactual transitions
<code>s'_{a}</code> and the counterfactual rewards <span
class="math inline">\(r_a\)</span>).</p>
<p>We play according to a randomly instantiated policy <span
class="math inline">\(\pi(s,a)\)</span></p>
<p><span class="math display">\[Q(s,a) = (1-\alpha)\,Q(s,a) + \alpha
\left( r_a  + \gamma Q(s&#39;_a)\right).\]</span></p>
<p>In addition, for each state <span class="math inline">\(s\)</span> we
calculate the actually observed action <span
class="math inline">\(a_{obs}\)</span></p>
<p><span class="math display">\[C(s, a_{obs}) += 1.\]</span></p>
<p>We can use these counts to keep track of our historical average
strategy</p>
<p><span class="math display">\[\bar{\pi}(s,a)  =  \frac{C(s, a)}{\sum_a
C(s,a)}\]</span></p>
<p>When we observe the other players reaction and the rewards and
consequences of all the actions we could have played according to our
strategy, we can try to establish which, <span
class="math inline">\(\pi(s,a)\)</span> or <span
class="math inline">\(\bar{\pi}(s,a)\)</span> is better, by averaging
over the expected value in state <span
class="math inline">\(s\)</span></p>
<p><span class="math display">\[V_{\pi}(s) = \sum_a \pi(s,a)
Q(s,a).\]</span></p>
<p>If <span class="math inline">\(V_{\pi}(s) &gt;
V_{\bar{\pi}}\)</span>, i.e. our current strategy is better than the
historic average, we use <span class="math inline">\(\delta =
\delta_w\)</span>, otherwise we use <span class="math inline">\(delta =
\delta_l &gt; \delta_w\)</span>. We update the action that maximises the
new value function</p>
<p><span class="math display">\[a_{max} = \max_{a} Q(s,a)\]</span></p>
<p>Then</p>
<p><span class="math display">\[\pi_{s,a_{max}} = \pi_{s,a_{max}} +
\delta\]</span> and all other actions get reduced in likelihood, whereas
probability mass is taken equally from all non-maximising actions</p>
<p><span class="math display">\[\pi_{s,a} = \pi_{s,a} -
\frac{\delta}{|A|-1}.\]</span></p>
<p>Also we have to clip and normalise to guarantee a correct probability
distribution for <span class="math inline">\(\pi(s,a)\)</span>, since
updates can in principle push <span
class="math inline">\(\pi(s,a)\)</span> outside the region <span
class="math inline">\([0,1]\)</span>. Because we constantly compare to
the average value of the long term strategy, when we are doing well, we
lower the learning rate and stay closer to the true equilibrium
value.</p>
</body>
</html>
